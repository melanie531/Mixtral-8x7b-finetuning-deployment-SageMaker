{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0174f16-bdd3-4cfd-a809-b32a4dc76c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.37.2)\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.10/site-packages (2.206.0)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (0.6.0)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.12.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting fsspec==2023.9.0\n",
      "  Downloading fsspec-2023.9.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: datasets==2.13.0 in /opt/conda/lib/python3.10/site-packages (from datasets[s3]==2.13.0) (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (1.26.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (3.8.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (0.20.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.0->datasets[s3]==2.13.0) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.33.3 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.33.13)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.21.12)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from sagemaker) (4.17.3)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from sagemaker) (3.5.1)\n",
      "Requirement already satisfied: tblib<3,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /opt/conda/lib/python3.10/site-packages (from sagemaker) (1.26.18)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from sagemaker) (7.0.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from sagemaker) (5.9.5)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2.9.0)\n",
      "INFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.12.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.10.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.7.0 (from s3fs)\n",
      "  Downloading aiobotocore-2.7.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.9.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiobotocore~=2.5.4 (from s3fs)\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting botocore<1.31.18,>=1.31.17 (from aiobotocore~=2.5.4->s3fs)\n",
      "  Downloading botocore-1.31.17-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.4->s3fs) (1.15.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.5.4->s3fs) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->datasets[s3]==2.13.0) (3.3.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->datasets[s3]==2.13.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->datasets[s3]==2.13.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->datasets[s3]==2.13.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->datasets[s3]==2.13.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.0->datasets[s3]==2.13.0) (1.3.1)\n",
      "INFO: pip is looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting boto3<2.0,>=1.33.3 (from sagemaker)\n",
      "  Downloading boto3-1.34.34-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.33-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.32-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.31-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.30-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.29-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.28-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: pip is still looking at multiple versions of boto3 to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading boto3-1.34.27-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.26-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.25-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.24-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.23-py3-none-any.whl.metadata (6.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading boto3-1.34.22-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.21-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.20-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.19-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.18-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.17-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.16-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.15-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.14-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.13-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.12-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.11-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.10-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.9-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.8-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.7-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.6-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.34.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "  Downloading boto3-1.33.12-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.11-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.10-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.9-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.8-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.7-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.6-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.5-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.4-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Downloading boto3-1.33.3-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: botocore<1.34.0,>=1.33.13 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.33.13)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.9.0,>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (0.8.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.0,>=1.33.13->boto3<2.0,>=1.33.3->sagemaker) (2.8.2)\n",
      "INFO: pip is still looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.6.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading s3fs-2023.5.0-py3-none-any.whl (28 kB)\n",
      "  Downloading s3fs-2023.4.0-py3-none-any.whl (28 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading s3fs-2023.3.0-py3-none-any.whl (27 kB)\n",
      "Collecting aiobotocore~=2.4.2 (from s3fs)\n",
      "  Downloading aiobotocore-2.4.2-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2023.1.0-py3-none-any.whl (27 kB)\n",
      "  Downloading s3fs-2022.11.0-py3-none-any.whl (27 kB)\n",
      "  Downloading s3fs-2022.10.0-py3-none-any.whl (27 kB)\n",
      "  Downloading s3fs-2022.8.2-py3-none-any.whl (27 kB)\n",
      "  Downloading s3fs-2022.8.1-py3-none-any.whl (27 kB)\n",
      "  Downloading s3fs-2022.8.0-py3-none-any.whl (27 kB)\n",
      "  Downloading s3fs-2022.7.1-py3-none-any.whl (27 kB)\n",
      "Collecting aiobotocore~=2.3.4 (from s3fs)\n",
      "  Downloading aiobotocore-2.3.4-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.7.0-py3-none-any.whl (27 kB)\n",
      "  Downloading s3fs-2022.5.0-py3-none-any.whl (27 kB)\n",
      "  Downloading s3fs-2022.3.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiobotocore~=2.2.0 (from s3fs)\n",
      "  Downloading aiobotocore-2.2.0.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2022.2.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiobotocore~=2.1.0 (from s3fs)\n",
      "  Downloading aiobotocore-2.1.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl (25 kB)\n",
      "  Downloading s3fs-2021.11.1-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=2.0.1 (from s3fs)\n",
      "  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.11.0-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=1.4.1 (from s3fs)\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.10.1-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.10.0-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.9.0-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.8.0-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.7.0-py3-none-any.whl (25 kB)\n",
      "  Downloading s3fs-2021.6.1-py3-none-any.whl (25 kB)\n",
      "  Downloading s3fs-2021.6.0-py3-none-any.whl (24 kB)\n",
      "  Downloading s3fs-2021.5.0-py3-none-any.whl (24 kB)\n",
      "  Downloading s3fs-2021.4.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.0->datasets[s3]==2.13.0) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.0->datasets[s3]==2.13.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.0->datasets[s3]==2.13.0) (2023.7.22)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->datasets[s3]==2.13.0) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.0->datasets[s3]==2.13.0) (2023.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Downloading fsspec-2023.9.0-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.12.2\n",
      "    Uninstalling fsspec-2023.12.2:\n",
      "      Successfully uninstalled fsspec-2023.12.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-ai 2.5.0 requires faiss-cpu, which is not installed.\n",
      "jupyter-scheduler 2.3.0 requires fsspec==2023.6.0, but you have fsspec 2023.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2023.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sagemaker s3fs \"fsspec==2023.9.0\" \"datasets[s3]==2.13.0\"  --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a583849d-1d2e-4017-ae51-2e4cda279617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker role arn: arn:aws:iam::631450739534:role/service-role/AmazonSageMaker-ExecutionRole-20231204T140306\n",
      "sagemaker bucket: sagemaker-us-east-1-631450739534\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "#gets role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c35dd900-8315-4576-8b56-c5fb611d24e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c771d4148394679bc100900347c3353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/databricks--databricks-dolly-15k to /home/sagemaker-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633ec713e0294a079a2d6fd5c5fbcf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4767e70d3ad34d30a7edc2969aad2444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e378be2b68e4643b693696445c03d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/sagemaker-user/.cache/huggingface/datasets/databricks___json/databricks--databricks-dolly-15k-7427aa6e57c34282/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "dataset size: 15011\n",
      "{'instruction': 'Suggest some sports I can do solo?', 'context': '', 'response': 'You can run, swim, cycle, dance - all by yourself.', 'category': 'brainstorming'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39aeb73f-1281-4592-87bb-2f8b4929d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(sample):\n",
    "    bos_token = \"<s>\"\n",
    "    system_message = \"[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n",
    "    instruction = sample[\"instruction\"]\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    question = \"\\n\".join([i for i in [instruction, context] if i is not None])\n",
    "    answer = sample['response']\n",
    "    eos_token = \"</s>\"\n",
    "\n",
    "    full_prompt = \"\"\n",
    "    full_prompt += bos_token\n",
    "    full_prompt += system_message\n",
    "    full_prompt += question\n",
    "    full_prompt += \" [/INST]\\n\\n\"\n",
    "    full_prompt += answer\n",
    "    full_prompt += eos_token\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b836550d-44ed-4dd0-9faa-00f2763f0761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\nWhat is a Pythagorean triple? [/INST]\\n\\nIn mathematics, a Pythagorean triple consists of three positive integers, a, b, c, such that a² + b² = c². These integers can form the sides of a right triangle, with c as the hypotenuse. For example, (3, 4, 5) is a Pythagorean triple because 3² + 4² = 5².</s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331dcbea-f4f3-4603-83c0-8a0e46ad0098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be7c522dbf54058a3014cfb4beec558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85673b3b045f4bdbbe4e043b6de46ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15fc0f087b344cf90d4504e565b29db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fcb553310f444c91570ca1ab31207a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "mixtral_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "mixtral_tokenizer.pad_token = mixtral_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8efb647b-a616-4f2f-bb6e-d52367c54245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "Extract the names of all of the albums that Taylor Swift has released. Separate them with a comma.\n",
      "### Context\n",
      "Swift signed a record deal with Big Machine Records in 2005 and released her eponymous debut album the following year. With 157 weeks on the Billboard 200 by December 2009, the album was the longest-charting album of the 2000s decade. Swift's second studio album, Fearless (2008), topped the Billboard 200 for 11 weeks and was the only album from the 2000s decade to spend one year in the top 10. The album was certified Diamond by the RIAA. It also topped charts in Australia and Canada, and has sold 12 million copies worldwide. Her third studio album, the self-written Speak Now (2010), spent six weeks atop the Billboard 200 and topped charts in Australia, Canada, and New Zealand.\n",
      "\n",
      "Her fourth studio album, Red (2012), was her first number-one album in the United Kingdom. It topped charts in Australia, Canada, Ireland, New Zealand, and spent seven weeks at number one on the Billboard 200. Swift scored her fourth US number-one album with 1989 (2014), which topped the Billboard 200 for 11 weeks and was certified 9× Platinum by the RIAA. It topped the charts in other countries including Australia, Canada, and New Zealand. Her sixth studio album, Reputation (2017), made Swift the first music artist to have four consecutive albums each sell over one million copies within its debut week. It spent four weeks atop the Billboard 200.\n",
      "\n",
      "Exiting Big Machine, Swift signed with Universal Music Group label Republic Records in 2018. Her seventh studio album, Lover (2019), was the year's global best-selling album by a solo artist. Swift released two studio albums in 2020, Folklore and Evermore, which respectively spent eight and four weeks atop the Billboard 200. Swift released two re-recorded albums, Fearless (Taylor's Version) and Red (Taylor's Version), in 2021, after a dispute with Big Machine over the rights to the masters of her first six albums; the former was the first re-recorded album to top the Billboard 200. Swift's tenth original studio album, Midnights (2022), became her fifth to sell over a million US first-week copies; It was also the first album to sell over a million physical sales since 2015. [/INST]\n",
      "\n",
      "Taylor Swift, Fearless, Speak Now, Red, 1989, Reputation, Lover, Fearless, Folklore, Evermore, Fearless (Taylor's Version), Red (Taylor's Version), Midnights</s>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15011 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{create_prompt(sample)}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "hf_df = dataset.map(template_dataset, remove_columns=list(dataset.features))\n",
    "# print random sample\n",
    "print(hf_df[randint(0, len(dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "lm_dataset = hf_df.map(\n",
    "    lambda sample: mixtral_tokenizer(sample[\"text\"]), batched=True, remove_columns=list(hf_df.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37a14d2d-80a2-41ce-91e2-0c869359d969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1695 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sagemaker_session_bucket}/train'\n",
    "local_path = './train_data'\n",
    "lm_dataset.save_to_disk(local_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8345ce9d-585b-4a01-91d0-45811e645e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: train_data/state.json to s3://sagemaker-us-east-1-631450739534/train/state.json\n",
      "upload: train_data/dataset_info.json to s3://sagemaker-us-east-1-631450739534/train/dataset_info.json\n",
      "upload: train_data/data-00000-of-00001.arrow to s3://sagemaker-us-east-1-631450739534/train/data-00000-of-00001.arrow\n",
      "uploaded data to:\n",
      "training dataset to: s3://sagemaker-us-east-1-631450739534/train\n"
     ]
    }
   ],
   "source": [
    "!aws s3 sync ./train_data $training_input_path\n",
    "print(\"uploaded data to:\")\n",
    "print(f\"training dataset to: {training_input_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "418ec5b0-46bc-443b-8828-d736f9adb371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# define Training Job Name\n",
    "job_name = f'huggingface-qlora-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters ={\n",
    "  'model_id': model_id,                             # pre-trained model\n",
    "  'dataset_path': '/opt/ml/input/data/training',    # path where sagemaker will save training dataset\n",
    "  'epochs': 1,                                      # number of training epochs\n",
    "  'per_device_train_batch_size': 1,                 # batch size for training\n",
    "  'lr': 2e-4,                                       # learning rate used during training\n",
    "  'hf_token': HfFolder.get_token(),                 # huggingface token to access llama 2\n",
    "  'merge_weights': True,                            # wether to merge LoRA into the model (needs more memory)\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'run_clm.py',      # train script\n",
    "    source_dir           = 'scripts',         # directory which includes all the files needed for training\n",
    "    instance_type        = 'ml.g5.12xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size          = 300,               # the size of the EBS volume in GB\n",
    "    transformers_version = '4.28',            # the transformers version used in the training job\n",
    "    pytorch_version      = '2.0',             # the pytorch_version version used in the training job\n",
    "    py_version           = 'py310',           # the python version used in the training job\n",
    "    hyperparameters      =  hyperparameters,  # the hyperparameters passed to the training job\n",
    "    environment          = { \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\" }, # set env variable to cache models in /tmp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2ea486-7f08-4930-b1cf-e97c9f110368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {'training': training_input_path}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e49e863a-4cac-4c5d-9c86-f72138e277ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training job finished with status Completed\n"
     ]
    }
   ],
   "source": [
    "response = sess.describe_training_job(\"huggingface-qlora-2024-02-04-11-16-22-2024-02-04-11-16-22-600\")\n",
    "if response[\"TrainingJobStatus\"] not in [\"Completed\", \"Faile\"]:\n",
    "    print(response[\"TrainingJobStatus\"])\n",
    "    time.sleep(60)\n",
    "else:\n",
    "    print(f'training job finished with status {response[\"TrainingJobStatus\"]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c5cd5-d2a8-49b1-9bea-7a164647ef0a",
   "metadata": {},
   "source": [
    "### Deploy fine-tuned model to SageMaker endpoints\n",
    "You can deploy your fine-tuned Mixtral 8x7b model to a SageMaker endpoint and use it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671eaa9-e834-40ea-a57a-678cf650bc90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
